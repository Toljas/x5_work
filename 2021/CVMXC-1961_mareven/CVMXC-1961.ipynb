{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd59562c",
   "metadata": {},
   "source": [
    "https://jira.x5.ru/browse/CVMXC-1961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4eeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'CVMXC-1961'\n",
    "seg_no = 'seg2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbfea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import List, Dict, Callable\n",
    "import socket\n",
    "\n",
    "spark = None\n",
    "\n",
    "EXECUTOR_ENV = 'hdfs:///share/products/cvm5/lib/python/anaconda_2.4.4_ds.tar.gz'  # 2.4.4 \n",
    "SPARK_ARCHIVE = 'hdfs:///share/lib/spark/sparkjars-2.4.4.zip'                     # 2.4.4\n",
    "#EXECUTOR_ENV = 'hdfs:///share/lib/python/env/anaconda-2019.07.tar.gz'\n",
    "#SPARK_ARCHIVE = 'hdfs:///share/lib/spark/sparkjars-2.3.1.zip'\n",
    "\n",
    "os.environ[\"ARROW_LIBHDFS_DIR\"] = \"/usr/hdp/2.6.5.0-292/usr/lib\"\n",
    "os.environ['HADOOP_HOME'] = '/usr/hdp/current/hadoop-client/'\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64/'\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf/'\n",
    "os.environ['SPARK_HOME'] = '/opt/conda/lib/python3.7/site-packages/pyspark'\n",
    "os.environ['PYSPARK_PYTHON'] = 'anaconda_2.4.4_ds.tar.gz/bin/python3'             # 2.4.4\n",
    "#os.environ['PYSPARK_PYTHON'] = 'anaconda-2019.07.tar.gz/bin/python3'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def restart_spark(task_name: str, num_executors: int, executor_memory='4G', executor_cores=2,\n",
    "                  driver_memory='2G', queue='bdse', additional_params: Dict[str, str] = None):\n",
    "    global spark\n",
    "\n",
    "    if spark:\n",
    "        sc = spark.sparkContext\n",
    "        if sc and sc._jsc:\n",
    "            if not sc._jsc.sc().isStopped():\n",
    "                print('Using cached spark')\n",
    "                return sc, spark\n",
    "\n",
    "    need_ports_for_app = 3\n",
    "    user_tcp_ports = _get_user_tcp_ports()\n",
    "    free_ports = _get_free_ports(user_tcp_ports)\n",
    "    assert len(free_ports) >= need_ports_for_app, \\\n",
    "        f\"Not enough free ports ({len(free_ports)}), need {need_ports_for_app}, stop other apps\"\n",
    "    app_ports = free_ports[:need_ports_for_app]\n",
    "\n",
    "    host_ip = os.getenv('HOST_IP')\n",
    "    \n",
    "    spark_session = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(task_name)\n",
    "        .master('yarn')\n",
    "        .config('spark.driver.memory', driver_memory)\n",
    "        .config('spark.driver.maxResultSize', driver_memory)\n",
    "        .config('spark.executor.cores', executor_cores)\n",
    "        .config('spark.executor.memory', executor_memory)\n",
    "        .config('spark.executor.memoryOverhead', '1G')\n",
    "        .config('spark.dynamicAllocation.enabled', 'true')\n",
    "        .config('spark.dynamicAllocation.maxExecutors', num_executors)\n",
    "        .config('spark.sql.broadcastTimeout', '36000')\n",
    "        .config('spark.dynamicAllocation.cachedExecutorIdleTimeout', '1200s')\n",
    "        .config('spark.ui.port', app_ports[0])\n",
    "        .config('spark.blockManager.port', app_ports[1])\n",
    "        .config('spark.driver.port', app_ports[2])\n",
    "        .config('spark.driver.host', host_ip)\n",
    "        .config('spark.driver.bindAddress', '0.0.0.0')\n",
    "        .config('spark.driver.extraLibraryPath', '/usr/hdp/2.6.5.0-292/hadoop/lib/native')\n",
    "        .config('spark.driver.extraJavaOptions', '-Dhdp.version=current')\n",
    "        .config('spark.debug.maxToStringFields', '50')\n",
    "        .config('spark.yarn.queue', queue)\n",
    "        .config('spark.yarn.dist.archives', EXECUTOR_ENV)\n",
    "        .config('spark.yarn.archive', SPARK_ARCHIVE)\n",
    "        .config('spark.yarn.am.extraJavaOptions', '-Dhdp.version=current')\n",
    "        .config('spark.rpc.message.maxSize', '1024')\n",
    "        .config('spark.sql.warehouse.dir', '/apps/hive/warehouse')\n",
    "        .config('spark.sql.execution.pandas.respectSessionTimeZone', 'false')\n",
    "        .config('spark.sql.orc.filterPushdown', 'true')\n",
    "        .config('spark.sql.hive.convertMetastoreOrc', 'true')\n",
    "        .config('spark.shuffle.service.enabled', 'true')\n",
    "        .config('spark.hadoop.yarn.timeline-service.enabled', 'false')\n",
    "        .config('spark.hadoop.yarn.client.failover-proxy-provider',\n",
    "                'org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider')\n",
    "        .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "        .config('spark.kryoserializer.buffer.max', '128m')\n",
    "        .config('spark.executor.extraLibraryPath', '/usr/hdp/2.6.5.0-292/hadoop/lib/native')\n",
    "    )\n",
    "\n",
    "    if additional_params:\n",
    "        for key, value in additional_params.items():\n",
    "            spark_session = spark_session.config(key, value)\n",
    "\n",
    "    spark = (\n",
    "        spark_session\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    return sc, spark\n",
    "\n",
    "\n",
    "def _get_user_tcp_ports() -> List[str]:\n",
    "    regexp = re.compile(r'-2e')\n",
    "    envuser= os.getenv('HOSTNAME')\n",
    "    if regexp.search(envuser):       \n",
    "      _, user_name, user_surname = envuser.upper().split('-')\n",
    "      user_full_name = '_'.join([user_name, user_surname])\n",
    "    else:\n",
    "      _, user_name  = envuser.upper().split('-') \n",
    "      user_full_name = user_name\n",
    "    user_tcp_ports = [v for k, v in os.environ.items() if user_full_name in k and k.endswith('TCP_PORT')]\n",
    "    return user_tcp_ports\n",
    "\n",
    "\n",
    "def _get_free_ports(ports: List[str]):\n",
    "    free_ports = []\n",
    "    for port in ports:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('0.0.0.0', int(port))) != 0:\n",
    "                free_ports.append(port)\n",
    "    return free_ports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f84895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc, spark = restart_spark(\n",
    "                    name, \n",
    "                    21, \n",
    "                    executor_memory='5G', \n",
    "                    executor_cores=3, \n",
    "                    driver_memory='7G', \n",
    "                    additional_params={\"spark.sql.shuffle.partitions\": \"300\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905f62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e308c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "sys.path.append('/home/jovyan/glow-byte-filters-pyspark')\n",
    "from logic_filters import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e93ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOYALTY_CARDS = \"hive_ssa_tc5.loyalty_card\"\n",
    "LOYALTY_CARDHOLDERS = \"hive_ssa_tc5.loyalty_cardholder\"\n",
    "ACCOUNTS = \"hive_ssa_tc5.account\"\n",
    "CVM5_GUESTS = \"hive_cvm_acrm.cvm5_guests\"\n",
    "\n",
    "DIM_STORE = \"hive_ssa_main.dim_store\"\n",
    "CHECKS_HEADERS = \"hive_ssa_main.fct_rtl_txn\"\n",
    "CHECKS_ITEMS = \"hive_ssa_main.fct_rtl_txn_item\"\n",
    "PRODUCTS = \"hive_ssa_tc5.cvm_product\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab745441",
   "metadata": {},
   "source": [
    "### Проверка на доступность плю в магазинах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c66842",
   "metadata": {},
   "outputs": [],
   "source": [
    "plu_codes = [45105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c2529d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22522"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pplu = spark.createDataFrame(pd.DataFrame({\"plu_code\": plu_codes}))\n",
    "\n",
    "shops = (spark.table(DIM_STORE)\n",
    "            .filter(F.col('valid_to_dttm')== datetime.datetime(5999, 1, 1, 0, 0))\n",
    "            #.filter(F.col('federal_subject_dk').isin([23, 93, 123, 193, 74, 174,\n",
    "             #                                         774, 16, 116, 716, 71, 32]))\n",
    "            .selectExpr('store_id as plant')\n",
    "            .distinct()\n",
    "            #.toPandas()['plant']\n",
    "            #.tolist()\n",
    "            )\n",
    "\n",
    "\n",
    "tc5_stores_assort = (\n",
    "    spark.table('HIVE_SSA_MAIN.ASSORTMENT_X_PLU_X_STORE')\n",
    "    .withColumnRenamed('plu_id', 'plu_code')\n",
    "    .withColumnRenamed('store_id', 'plant')\n",
    "    .filter(F.col(\"plu_negate_flg\") != 1)\n",
    "    .selectExpr('plu_code', 'plant')\n",
    "    .join(F.broadcast(pplu), 'plu_code', 'inner')\n",
    ")\n",
    "\n",
    "plants = (shops.join(tc5_stores_assort, on='plant', how='inner')\n",
    "                .select('plant')\n",
    "                .distinct()\n",
    "                .toPandas()['plant']\n",
    "                .tolist()\n",
    "         )\n",
    "\n",
    "len(plants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f64a0",
   "metadata": {},
   "source": [
    "### Выбираем гостей нужного юзкейса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a935ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase = ['cross', 'upgrade', 'ump']\n",
    "lifetime = 90\n",
    "freq = 1\n",
    "dt = datetime.date.today()\n",
    "# dt = datetime.date(2021, 11, 13)\n",
    "# plu_hierarchy_lvl_4_dk = 'FD0216001'\n",
    "syntethic_category_id = [73]\n",
    "base_qty_per_period = 1\n",
    "plu_codes_cat = [3680376, 3623999, 45105, 3680375, 3624002, 3623997, 3623998, 3624001, 4081735, 3680377, 3179905]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "408f00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_usecase = (spark\n",
    "                     .table(CVM5_GUESTS)\n",
    "                     .filter(F.col('calculation_dt') >= dt)\n",
    "                     .filter(F.col('usecase').isin(usecase))\n",
    "                     .filter(F.col('lifetime') >= lifetime)\n",
    "                     .filter(F.col('frequency') >= freq)\n",
    "                     .select('account_no', 'customer_rk')\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3202fb",
   "metadata": {},
   "source": [
    "### Собираем чеки этих гостей и оставляем тех, кто покупал продукты с synthetic_catalog_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c95373",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date1 = datetime.date(2021, 5, 10)\n",
    "end_date1 = datetime.date(2021, 8, 9)\n",
    "start_date2 = datetime.date(2021, 8, 10)\n",
    "end_date2 = datetime.date(2021, 11, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fa6c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers = (spark.table(CHECKS_HEADERS)\n",
    "                      .filter(F.col('rtl_txn_dt').between(start_date1, end_date1))\n",
    "#                       .filter(F.col('hour_num') == 0)\n",
    "                      .filter((F.col('loyalty_card_no') != '') & (F.col('loyalty_card_no').isNotNull()))\n",
    "                      .filter(F.col('financial_unit_format_dk') == 'D')\n",
    "                      .filter(F.col('rtl_txn_cancel_flg') == 0)\n",
    "                      .select('rtl_txn_id', 'loyalty_card_no', 'store_id')\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c88877",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers_tc5 = checks_headers.filter(F.col('store_id').isin(plants)) #оставили чеки только с нужными магазинами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcb489db",
   "metadata": {},
   "outputs": [],
   "source": [
    "loyalty_cards = (spark\n",
    "                    .table(LOYALTY_CARDS)\n",
    "                    .withColumnRenamed(\"loyalty_card_id\", \"loyalty_card_no\")\n",
    "                    .withColumnRenamed(\"loyalty_account_id\", \"account_no\")\n",
    "                    .withColumnRenamed(\"loyalty_account_acrm_id\", \"account_rk\")\n",
    "                    .select('account_no', 'loyalty_card_no')\n",
    "                )\n",
    "loyalty_cardholders = (spark\n",
    "                        .table(LOYALTY_CARDHOLDERS)\n",
    "                        .withColumnRenamed(\"loyalty_cardholder_acrm_id\", \"customer_rk\")\n",
    "                        .withColumnRenamed(\"loyalty_account_id\", \"account_no\")\n",
    "                        .select('account_no', 'customer_rk')\n",
    "                      )\n",
    "clients_info = loyalty_cards.join(loyalty_cardholders, on='account_no', how='inner')\n",
    "clients_info = clients_info.join(customers_usecase, on=['account_no', 'customer_rk'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c87aa997",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers_tc5 = checks_headers_tc5.join(clients_info, on='loyalty_card_no') #оставили чеки только нужных гостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b23b8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_items = (spark.table(CHECKS_ITEMS) \n",
    "                    .withColumnRenamed('plu_id', 'plu_code')\n",
    "                    .withColumnRenamed('turnover_no_vat_amt', 'zsalnovat')\n",
    "                    .withColumnRenamed('turnover_vat_rub_amt', 'zsale_vat')\n",
    "                    .withColumnRenamed('prime_cost_no_vat_amt', 'zcst_n')\n",
    "                    .withColumnRenamed('turnover_base_uom_amt', 'base_qty')\n",
    "                    .withColumnRenamed('discount_amt', 'zdiscount')\n",
    "                    .withColumnRenamed('fact_regular_promo_flg', 'zpromofl')\n",
    "                    .filter(F.col('rtl_txn_dt').between(start_date1, end_date1))\n",
    "                    .filter((F.col('zsalnovat') >= 0) & (F.col('base_qty') >= 0) & (F.col('zcst_n') > 0)) #keep only correct data\n",
    "                        .select('plu_code' #items id\n",
    "                                , 'rtl_txn_id' #cheques id\n",
    "                                , 'base_qty' #count\n",
    "                               )\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "add2cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_items = checks_items.filter(F.col('plu_code').isin(plu_codes_cat)) # только чеки с нужными plu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3002291",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_tc5 = checks_items.join(checks_headers_tc5, 'rtl_txn_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52df8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = ['account_no']\n",
    "pdf = checks_tc5.groupby(accs).agg(F.sum('base_qty').alias('base_qty_per_period'))\n",
    "pdf = pdf.filter(F.col('base_qty_per_period') >= base_qty_per_period)\n",
    "checks_tc5 = checks_tc5.join(pdf, on=accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9414538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = ( checks_tc5.select('customer_rk')\n",
    "                  .distinct()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75e99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc4e35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers2 = (spark.table(CHECKS_HEADERS)\n",
    "                      .filter(F.col('rtl_txn_dt').between(start_date2, end_date2))\n",
    "#                       .filter(F.col('hour_num') == 0)\n",
    "                      .filter((F.col('loyalty_card_no') != '') & (F.col('loyalty_card_no').isNotNull()))\n",
    "                      .filter(F.col('financial_unit_format_dk') == 'D')\n",
    "                      .filter(F.col('rtl_txn_cancel_flg') == 0)\n",
    "                      .select('rtl_txn_id', 'loyalty_card_no', 'store_id')\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99030164",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers_tc52 = checks_headers2.filter(F.col('store_id').isin(plants)) #оставили чеки только с нужными магазинами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a33c5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "loyalty_cards2 = (spark\n",
    "                    .table(LOYALTY_CARDS)\n",
    "                    .withColumnRenamed(\"loyalty_card_id\", \"loyalty_card_no\")\n",
    "                    .withColumnRenamed(\"loyalty_account_id\", \"account_no\")\n",
    "                    .withColumnRenamed(\"loyalty_account_acrm_id\", \"account_rk\")\n",
    "                    .select('account_no', 'loyalty_card_no')\n",
    "                )\n",
    "loyalty_cardholders2 = (spark\n",
    "                        .table(LOYALTY_CARDHOLDERS)\n",
    "                        .withColumnRenamed(\"loyalty_cardholder_acrm_id\", \"customer_rk\")\n",
    "                        .withColumnRenamed(\"loyalty_account_id\", \"account_no\")\n",
    "                        .select('account_no', 'customer_rk')\n",
    "                      )\n",
    "clients_info2 = loyalty_cards2.join(loyalty_cardholders2, on='account_no', how='inner')\n",
    "clients_info2 = clients_info2.join(customers_usecase, on=['account_no', 'customer_rk'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cf39e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers_tc52 = checks_headers_tc52.join(clients_info2, on='loyalty_card_no') #оставили чеки только нужных гостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f7bea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_items2 = (spark.table(CHECKS_ITEMS) \n",
    "                    .withColumnRenamed('plu_id', 'plu_code')\n",
    "                    .withColumnRenamed('turnover_no_vat_amt', 'zsalnovat')\n",
    "                    .withColumnRenamed('turnover_vat_rub_amt', 'zsale_vat')\n",
    "                    .withColumnRenamed('prime_cost_no_vat_amt', 'zcst_n')\n",
    "                    .withColumnRenamed('turnover_base_uom_amt', 'base_qty')\n",
    "                    .withColumnRenamed('discount_amt', 'zdiscount')\n",
    "                    .withColumnRenamed('fact_regular_promo_flg', 'zpromofl')\n",
    "                    .filter(F.col('rtl_txn_dt').between(start_date2, end_date2))\n",
    "                    .filter((F.col('zsalnovat') >= 0) & (F.col('base_qty') >= 0) & (F.col('zcst_n') > 0)) #keep only correct data\n",
    "                        .select('plu_code' #items id\n",
    "                                , 'rtl_txn_id' #cheques id\n",
    "                                , 'base_qty' #count\n",
    "                               )\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19f75e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plu_codes_cat2 = (spark\n",
    "                 .table(PRODUCTS)\n",
    "                 .filter(~F.col('plu_id').isin(plu_codes_cat))\n",
    "#                  .filter(F.col('plu_hierarchy_lvl_4_dk') == plu_hierarchy_lvl_4_dk)\n",
    "#                  .filter(F.col('plu_brand_code') != brand_code)\n",
    "                 .filter(F.col('syntethic_category_id').isin(syntethic_category_id))\n",
    "                 .select('plu_id')\n",
    "                 .distinct()\n",
    "                 .toPandas()['plu_id']\n",
    "                 .tolist()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b23972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_items2 = checks_items2.filter(F.col('plu_code').isin(plu_codes_cat2)) # только чеки с нужными plu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aadf9f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_tc52 = checks_items2.join(checks_headers_tc52, 'rtl_txn_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dcf3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = ['account_no']\n",
    "pdf2 = checks_tc52.groupby(accs).agg(F.sum('base_qty').alias('base_qty_per_period'))\n",
    "pdf2 = pdf2.filter(F.col('base_qty_per_period') >= base_qty_per_period)\n",
    "checks_tc52 = checks_tc52.join(pdf2, on=accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f67f5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg2 = ( checks_tc52.select('customer_rk')\n",
    "                  .distinct()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3c825bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_all = seg2.join(seg, 'customer_rk', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c347c6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/14 20:10:48 ERROR client.TransportResponseHandler: Still have 1 requests outstanding when connection from /192.168.234.67:48636 is closed\n",
      "21/11/14 20:10:48 ERROR client.TransportResponseHandler: Still have 1 requests outstanding when connection from /192.168.234.67:56804 is closed\n",
      "21/11/14 20:10:49 ERROR client.TransportResponseHandler: Still have 1 requests outstanding when connection from /192.168.234.67:33966 is closed\n",
      "21/11/14 20:10:49 ERROR client.TransportResponseHandler: Still have 1 requests outstanding when connection from /192.168.234.67:59480 is closed\n",
      "21/11/14 20:10:49 ERROR client.TransportResponseHandler: Still have 1 requests outstanding when connection from /192.168.234.67:38530 is closed\n",
      "21/11/14 20:10:49 ERROR client.TransportResponseHandler: Still have 1 requests outstanding when connection from /192.168.234.67:43470 is closed\n"
     ]
    }
   ],
   "source": [
    "seg_all.write.parquet('temp', mode='overwrite')\n",
    "seg_all = spark.read.parquet('temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5eae6a",
   "metadata": {},
   "source": [
    "### Проверяем на доступность отобранных гостей в определенную дату"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c94127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_date = '2021-11-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b33612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m Время выполнения: 0:00:14\n"
     ]
    }
   ],
   "source": [
    "seg_sms1 = (sms_channel_filters_glowbyte(spark=spark,\n",
    "                                         guests=seg_all, \n",
    "                                         usecase_name=usecase[0], \n",
    "                                         check_date=check_date, \n",
    "                                         debug_mode=False)\n",
    "                                    .select('customer_rk')\n",
    "                                    .distinct()\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93d4869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m Время выполнения: 0:00:11\n"
     ]
    }
   ],
   "source": [
    "seg_sms2 = (sms_channel_filters_glowbyte(spark=spark,\n",
    "                                         guests=seg_all, \n",
    "                                         usecase_name=usecase[1], \n",
    "                                         check_date=check_date, \n",
    "                                         debug_mode=False)\n",
    "                                    .select('customer_rk')\n",
    "                                    .distinct()\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "283bd261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m Время выполнения: 0:00:20\n"
     ]
    }
   ],
   "source": [
    "seg_sms3 = (sms_channel_filters_glowbyte(spark=spark,\n",
    "                                         guests=seg_all, \n",
    "                                         usecase_name=usecase[2], \n",
    "                                         check_date=check_date, \n",
    "                                         debug_mode=False)\n",
    "                                    .select('customer_rk')\n",
    "                                    .distinct()\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6da527ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_sms = seg_sms1.union(seg_sms2).union(seg_sms3).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11c1c3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/14 20:12:20 ERROR client.TransportClient: Failed to send RPC RPC 4673346405139917215 to /192.168.234.67:44396: java.nio.channels.ClosedChannelException\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)\n",
      "                                                                                90]]]\r"
     ]
    }
   ],
   "source": [
    "seg_sms.write.parquet('temp01', mode='overwrite')\n",
    "seg_sms = spark.read.parquet('temp01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29d330e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "segm = (seg_all.join(seg_sms, on='customer_rk', how='inner')\n",
    "                .select('customer_rk')\n",
    "                .distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af2fd8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "seg_sms_pd = segm.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb0cb586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202634"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seg_sms_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7de4c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_sms_pd.to_csv(name + '_' + seg_no + '_All_uc' + str(dt) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a3147bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_sms_pd = seg_sms_pd.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbfa0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = seg_sms_pd.loc[(seg_sms_pd['customer_rk'].str.endswith('1')) | (seg_sms_pd['customer_rk'].str.endswith('2'))\n",
    "              | (seg_sms_pd['customer_rk'].str.endswith('3')) | (seg_sms_pd['customer_rk'].str.endswith('4'))].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b2fe28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "upgrade = seg_sms_pd.loc[(seg_sms_pd['customer_rk'].str.endswith('5')) | (seg_sms_pd['customer_rk'].str.endswith('6'))\n",
    "              | (seg_sms_pd['customer_rk'].str.endswith('7')) | (seg_sms_pd['customer_rk'].str.endswith('8'))].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bf0bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ump = seg_sms_pd.loc[(seg_sms_pd['customer_rk'].str.endswith('0'))\n",
    "              | (seg_sms_pd['customer_rk'].str.endswith('9'))].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d23c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross.to_csv(name + '_' + seg_no + '_' + usecase[0] + '_' + str(dt) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e06dff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "upgrade.to_csv(name + '_' + seg_no + '_' + usecase[1] + '_' + str(dt) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a8c7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ump.to_csv(name + '_' + seg_no + '_' + usecase[2] + '_' + str(dt) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53eacb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross: 48071, upgrade: 99504, ump: 55059\n"
     ]
    }
   ],
   "source": [
    "print('cross: {}, upgrade: {}, ump: {}'.format(len(cross), len(upgrade), len(ump)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af4730",
   "metadata": {},
   "source": [
    "https://jira.x5.ru/browse/CVMXC-1961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49b9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'CVMXC-1961_mareven'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b70ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import List, Dict, Callable\n",
    "import socket\n",
    "\n",
    "spark = None\n",
    "\n",
    "EXECUTOR_ENV = 'hdfs:///share/products/cvm5/lib/python/anaconda_2.4.4_ds.tar.gz'  # 2.4.4 \n",
    "SPARK_ARCHIVE = 'hdfs:///share/lib/spark/sparkjars-2.4.4.zip'                     # 2.4.4\n",
    "#EXECUTOR_ENV = 'hdfs:///share/lib/python/env/anaconda-2019.07.tar.gz'\n",
    "#SPARK_ARCHIVE = 'hdfs:///share/lib/spark/sparkjars-2.3.1.zip'\n",
    "\n",
    "os.environ[\"ARROW_LIBHDFS_DIR\"] = \"/usr/hdp/2.6.5.0-292/usr/lib\"\n",
    "os.environ['HADOOP_HOME'] = '/usr/hdp/current/hadoop-client/'\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64/'\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf/'\n",
    "os.environ['SPARK_HOME'] = '/opt/conda/lib/python3.7/site-packages/pyspark'\n",
    "os.environ['PYSPARK_PYTHON'] = 'anaconda_2.4.4_ds.tar.gz/bin/python3'             # 2.4.4\n",
    "#os.environ['PYSPARK_PYTHON'] = 'anaconda-2019.07.tar.gz/bin/python3'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def restart_spark(task_name: str, num_executors: int, executor_memory='4G', executor_cores=2,\n",
    "                  driver_memory='2G', queue='cvm5-rnd', additional_params: Dict[str, str] = None):\n",
    "    global spark\n",
    "\n",
    "    if spark:\n",
    "        sc = spark.sparkContext\n",
    "        if sc and sc._jsc:\n",
    "            if not sc._jsc.sc().isStopped():\n",
    "                print('Using cached spark')\n",
    "                return sc, spark\n",
    "\n",
    "    need_ports_for_app = 3\n",
    "    user_tcp_ports = _get_user_tcp_ports()\n",
    "    free_ports = _get_free_ports(user_tcp_ports)\n",
    "    assert len(free_ports) >= need_ports_for_app, \\\n",
    "        f\"Not enough free ports ({len(free_ports)}), need {need_ports_for_app}, stop other apps\"\n",
    "    app_ports = free_ports[:need_ports_for_app]\n",
    "\n",
    "    host_ip = os.getenv('HOST_IP')\n",
    "    \n",
    "    spark_session = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(task_name)\n",
    "        .master('yarn')\n",
    "        .config('spark.driver.memory', driver_memory)\n",
    "        .config('spark.driver.maxResultSize', driver_memory)\n",
    "        .config('spark.executor.cores', executor_cores)\n",
    "        .config('spark.executor.memory', executor_memory)\n",
    "        .config('spark.executor.memoryOverhead', '1G')\n",
    "        .config('spark.dynamicAllocation.enabled', 'true')\n",
    "        .config('spark.dynamicAllocation.maxExecutors', num_executors)\n",
    "        .config('spark.sql.broadcastTimeout', '36000')\n",
    "        .config('spark.dynamicAllocation.cachedExecutorIdleTimeout', '1200s')\n",
    "        .config('spark.ui.port', app_ports[0])\n",
    "        .config('spark.blockManager.port', app_ports[1])\n",
    "        .config('spark.driver.port', app_ports[2])\n",
    "        .config('spark.driver.host', host_ip)\n",
    "        .config('spark.driver.bindAddress', '0.0.0.0')\n",
    "        .config('spark.driver.extraLibraryPath', '/usr/hdp/2.6.5.0-292/hadoop/lib/native')\n",
    "        .config('spark.driver.extraJavaOptions', '-Dhdp.version=current')\n",
    "        .config('spark.debug.maxToStringFields', '50')\n",
    "        .config('spark.yarn.queue', queue)\n",
    "        .config('spark.yarn.dist.archives', EXECUTOR_ENV)\n",
    "        .config('spark.yarn.archive', SPARK_ARCHIVE)\n",
    "        .config('spark.yarn.am.extraJavaOptions', '-Dhdp.version=current')\n",
    "        .config('spark.rpc.message.maxSize', '1024')\n",
    "        .config('spark.sql.warehouse.dir', '/apps/hive/warehouse')\n",
    "        .config('spark.sql.execution.pandas.respectSessionTimeZone', 'false')\n",
    "        .config('spark.sql.orc.filterPushdown', 'true')\n",
    "        .config('spark.sql.hive.convertMetastoreOrc', 'true')\n",
    "        .config('spark.shuffle.service.enabled', 'true')\n",
    "        .config('spark.hadoop.yarn.timeline-service.enabled', 'false')\n",
    "        .config('spark.hadoop.yarn.client.failover-proxy-provider',\n",
    "                'org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider')\n",
    "        .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "        .config('spark.kryoserializer.buffer.max', '128m')\n",
    "        .config('spark.executor.extraLibraryPath', '/usr/hdp/2.6.5.0-292/hadoop/lib/native')\n",
    "    )\n",
    "\n",
    "    if additional_params:\n",
    "        for key, value in additional_params.items():\n",
    "            spark_session = spark_session.config(key, value)\n",
    "\n",
    "    spark = (\n",
    "        spark_session\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    return sc, spark\n",
    "\n",
    "\n",
    "def _get_user_tcp_ports() -> List[str]:\n",
    "    regexp = re.compile(r'-2e')\n",
    "    envuser= os.getenv('HOSTNAME')\n",
    "    if regexp.search(envuser):       \n",
    "      _, user_name, user_surname = envuser.upper().split('-')\n",
    "      user_full_name = '_'.join([user_name, user_surname])\n",
    "    else:\n",
    "      _, user_name  = envuser.upper().split('-') \n",
    "      user_full_name = user_name\n",
    "    user_tcp_ports = [v for k, v in os.environ.items() if user_full_name in k and k.endswith('TCP_PORT')]\n",
    "    return user_tcp_ports\n",
    "\n",
    "\n",
    "def _get_free_ports(ports: List[str]):\n",
    "    free_ports = []\n",
    "    for port in ports:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('0.0.0.0', int(port))) != 0:\n",
    "                free_ports.append(port)\n",
    "    return free_ports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fed2c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/29 12:24:03 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "sc, spark = restart_spark(\n",
    "                    name, \n",
    "                    21, \n",
    "                    executor_memory='5G', \n",
    "                    executor_cores=3, \n",
    "                    driver_memory='7G', \n",
    "                    additional_params={\"spark.sql.shuffle.partitions\": \"300\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "880546b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0decf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "sys.path.append('/home/jovyan/glow-byte-filters-pyspark')\n",
    "from logic_filters import * \n",
    "from segmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0e4552",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOYALTY_CARDS = \"hive_ssa_tc5.loyalty_card\"\n",
    "LOYALTY_CARDHOLDERS = \"hive_ssa_tc5.loyalty_cardholder\"\n",
    "ACCOUNTS = \"hive_ssa_tc5.account\"\n",
    "CVM5_GUESTS = \"hive_cvm_acrm.cvm5_guests\"\n",
    "\n",
    "DIM_STORE = \"hive_ssa_main.dim_store\"\n",
    "CHECKS_HEADERS = \"hive_ssa_main.fct_rtl_txn\"\n",
    "CHECKS_ITEMS = \"hive_ssa_main.fct_rtl_txn_item\"\n",
    "PRODUCTS = \"hive_ssa_tc5.cvm_product\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d296760",
   "metadata": {},
   "source": [
    "### Проверка на доступность плю в магазинах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8f995",
   "metadata": {},
   "source": [
    "GEO - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957a1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# federal_subject = [22, 42, 54]\n",
    "# macroregion_dk = ['MRDND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f7a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "plu_codes = [3680376, 3623999, 45105, 3680375, 3624002, 3623997, 3623998, 3624001, 4081735, 3680377, 3179905]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8743a223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22664"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pplu = spark.createDataFrame(pd.DataFrame({\"plu_code\": plu_codes}))\n",
    "\n",
    "shops = (spark.table(DIM_STORE)\n",
    "            .filter(F.col('valid_to_dttm')== datetime.datetime(5999, 1, 1, 0, 0))\n",
    "#             .filter(F.col('federal_subject_dk').isin(federal_subject))\n",
    "#             .filter(F.col('macroregion_dk').isin(macroregion_dk))\n",
    "            .selectExpr('store_id as plant')\n",
    "            .distinct()\n",
    "            #.toPandas()['plant']\n",
    "            #.tolist()\n",
    "            )\n",
    "\n",
    "\n",
    "tc5_stores_assort = (\n",
    "    spark.table('HIVE_SSA_MAIN.ASSORTMENT_X_PLU_X_STORE')\n",
    "    .withColumnRenamed('plu_id', 'plu_code')\n",
    "    .withColumnRenamed('store_id', 'plant')\n",
    "    .filter(F.col(\"plu_negate_flg\") != 1)\n",
    "    .selectExpr('plu_code', 'plant')\n",
    "    .join(F.broadcast(pplu), 'plu_code', 'inner')\n",
    ")\n",
    "\n",
    "plants = (shops.join(tc5_stores_assort, on='plant', how='inner')\n",
    "                .select('plant')\n",
    "                .distinct()\n",
    "                .toPandas()['plant']\n",
    "                .tolist()\n",
    "         )\n",
    "\n",
    "len(plants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c45704",
   "metadata": {},
   "source": [
    "### Выбираем гостей нужного юзкейса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea5b3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase = ['cross', 'upgrade', 'ump']\n",
    "lifetime = 90\n",
    "freq = 1\n",
    "dt = datetime.date.today()\n",
    "start_date1 = datetime.date(2021, 3, 25)\n",
    "end_date1 = datetime.date(2021, 8, 24)\n",
    "start_date2 = datetime.date(2021, 8, 25)\n",
    "end_date2 = datetime.date(2021, 11, 24)\n",
    "base_qty_per_period = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b87958ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_usecase = (spark\n",
    "                     .table(CVM5_GUESTS)\n",
    "                     .filter(F.col('calculation_dt') >= dt)\n",
    "                     .filter(F.col('usecase').isin(usecase))\n",
    "                     .filter(F.col('lifetime') >= lifetime)\n",
    "                     .filter(F.col('frequency') >= freq)\n",
    "                     .select('account_no', 'customer_rk')\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d47de1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15927530"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_usecase.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc29b2",
   "metadata": {},
   "source": [
    "### Проверяем на доступность отобранных гостей в определенную дату"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421b04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_date = '2022-02-02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6786b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m Время выполнения: 0:00:10\n"
     ]
    }
   ],
   "source": [
    "seg_sms1 = (sms_channel_filters_glowbyte(spark=spark,\n",
    "                                         guests=customers_usecase, \n",
    "                                         usecase_name=usecase[0], \n",
    "                                         check_date=check_date, \n",
    "                                         debug_mode=False)\n",
    "                                    .select('customer_rk')\n",
    "                                    .distinct()\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f4efb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m Время выполнения: 0:00:10\n"
     ]
    }
   ],
   "source": [
    "seg_sms2 = (sms_channel_filters_glowbyte(spark=spark,\n",
    "                                         guests=customers_usecase, \n",
    "                                         usecase_name=usecase[1], \n",
    "                                         check_date=check_date, \n",
    "                                         debug_mode=False)\n",
    "                                    .select('customer_rk')\n",
    "                                    .distinct()\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "845cc1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m Время выполнения: 0:00:08\n"
     ]
    }
   ],
   "source": [
    "seg_sms3 = (sms_channel_filters_glowbyte(spark=spark,\n",
    "                                         guests=customers_usecase, \n",
    "                                         usecase_name=usecase[2], \n",
    "                                         check_date=check_date, \n",
    "                                         debug_mode=False)\n",
    "                                    .select('customer_rk')\n",
    "                                    .distinct()\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3b8b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_sms = seg_sms1.union(seg_sms2).union(seg_sms3).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a94bc2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                840]]40]]\r"
     ]
    }
   ],
   "source": [
    "seg_sms.write.parquet('temp1961_1', mode='overwrite')\n",
    "seg_sms = spark.read.parquet('temp1961_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f3cc460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4246157"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_sms.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766c34a",
   "metadata": {},
   "source": [
    "# seg1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadffd25",
   "metadata": {},
   "source": [
    "### Собираем чеки этих гостей и оставляем тех, кто покупал продукты с synthetic_catalog_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7149bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers = (spark.table(CHECKS_HEADERS)\n",
    "                      .filter(F.col('rtl_txn_dt').between(start_date1, end_date1))\n",
    "                      .filter((F.col('loyalty_card_no') != '') & (F.col('loyalty_card_no').isNotNull()))\n",
    "                      .filter(F.col('financial_unit_format_dk') == 'D')\n",
    "                      .filter(F.col('rtl_txn_cancel_flg') == 0)\n",
    "                      .select('rtl_txn_id', 'loyalty_card_no', 'store_id')\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcab40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers_tc5 = checks_headers.filter(F.col('store_id').isin(plants)) #оставили чеки только с нужными магазинами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e69d6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "loyalty_cards = (spark\n",
    "                    .table(LOYALTY_CARDS)\n",
    "                    .withColumnRenamed(\"loyalty_card_id\", \"loyalty_card_no\")\n",
    "                    .withColumnRenamed(\"loyalty_account_id\", \"account_no\")\n",
    "                    .withColumnRenamed(\"loyalty_account_acrm_id\", \"account_rk\")\n",
    "                    .select('account_no', 'loyalty_card_no')\n",
    "                )\n",
    "loyalty_cardholders = (spark\n",
    "                        .table(LOYALTY_CARDHOLDERS)\n",
    "#                         .filter(F.col('gender_dk').isin(['F', 'U']))\n",
    "                        .withColumnRenamed(\"loyalty_cardholder_acrm_id\", \"customer_rk\")\n",
    "                        .withColumnRenamed(\"loyalty_account_id\", \"account_no\")\n",
    "                        .select('account_no', 'customer_rk')\n",
    "                      )\n",
    "clients_info = loyalty_cards.join(loyalty_cardholders, on='account_no', how='inner')\n",
    "clients_info = clients_info.join(seg_sms, on='customer_rk', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40715e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers_tc5 = checks_headers_tc5.join(clients_info, on='loyalty_card_no') #оставили чеки только нужных гостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d042677",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_items = (spark.table(CHECKS_ITEMS) \n",
    "                    .withColumnRenamed('plu_id', 'plu_code')\n",
    "                    .withColumnRenamed('turnover_no_vat_amt', 'zsalnovat')\n",
    "                    .withColumnRenamed('turnover_vat_rub_amt', 'zsale_vat')\n",
    "                    .withColumnRenamed('prime_cost_no_vat_amt', 'zcst_n')\n",
    "                    .withColumnRenamed('turnover_base_uom_amt', 'base_qty')\n",
    "                    .withColumnRenamed('discount_amt', 'zdiscount')\n",
    "                    .withColumnRenamed('fact_regular_promo_flg', 'zpromofl')\n",
    "                    .filter(F.col('rtl_txn_dt').between(start_date1, end_date1))\n",
    "                    .filter((F.col('zsalnovat') >= 0) & (F.col('base_qty') >= 0) & (F.col('zcst_n') > 0)) #keep only correct data\n",
    "                        .select('plu_code' #items id\n",
    "                                , 'rtl_txn_id' #cheques id\n",
    "                                , 'base_qty' #count\n",
    "                               )\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba95bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plu_hierarchy_lvl_4_dk = ['FR0405002']\n",
    "# syntethic_category_id = [42]\n",
    "# plu_brand_code = ['L473','2945','2657','3624','6390','6416']\n",
    "# plu_not_in = [4138521]\n",
    "plu_id = [3680376, 3623999, 45105, 3680375, 3624002, 3623997, 3623998, 3624001, 4081735, 3680377, 3179905]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf599738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plu_codes_cat = (spark\n",
    "                 .table(PRODUCTS)\n",
    "#                  .filter(F.col('plu_hierarchy_lvl_4_dk').isin(plu_hierarchy_lvl_4_dk))\n",
    "#                  .filter(F.col('plu_brand_code').isin(plu_brand_code))\n",
    "#                  .filter(F.col('syntethic_category_id').isin(syntethic_category_id))\n",
    "#                  .filter(~F.col('plu_id').isin(plu_not_in))\n",
    "                 .filter(F.col('plu_id').isin(plu_id))\n",
    "                 .select('plu_id')\n",
    "                 .distinct()\n",
    "                 .toPandas()['plu_id']\n",
    "                 .tolist()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98ef0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_items = checks_items.filter(F.col('plu_code').isin(plu_codes_cat)) # только чеки с нужными plu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b46369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_tc5 = checks_items.join(checks_headers_tc5, 'rtl_txn_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e979dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = ['account_no']\n",
    "pdf = checks_tc5.groupby(accs).agg(F.sum('base_qty').alias('base_qty_per_period'))\n",
    "pdf = pdf.filter(F.col('base_qty_per_period') >= base_qty_per_period)\n",
    "checks_tc5 = checks_tc5.join(pdf, on=accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f339fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg1 = (checks_tc5.select('customer_rk')\n",
    "                  .distinct()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ab44160",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers_2 = (spark.table(CHECKS_HEADERS)\n",
    "                      .filter(F.col('rtl_txn_dt').between(start_date2, end_date2))\n",
    "                      .filter((F.col('loyalty_card_no') != '') & (F.col('loyalty_card_no').isNotNull()))\n",
    "                      .filter(F.col('financial_unit_format_dk') == 'D')\n",
    "                      .filter(F.col('rtl_txn_cancel_flg') == 0)\n",
    "                      .select('rtl_txn_id', 'loyalty_card_no', 'store_id')\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5090e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers_tc5_2 = checks_headers_2.filter(F.col('store_id').isin(plants)) #оставили чеки только с нужными магазинами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c80315e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loyalty_cards = (spark\n",
    "                    .table(LOYALTY_CARDS)\n",
    "                    .withColumnRenamed(\"loyalty_card_id\", \"loyalty_card_no\")\n",
    "                    .withColumnRenamed(\"loyalty_account_id\", \"account_no\")\n",
    "                    .withColumnRenamed(\"loyalty_account_acrm_id\", \"account_rk\")\n",
    "                    .select('account_no', 'loyalty_card_no')\n",
    "                )\n",
    "loyalty_cardholders = (spark\n",
    "                        .table(LOYALTY_CARDHOLDERS)\n",
    "#                         .filter(F.col('gender_dk').isin(['F', 'U']))\n",
    "                        .withColumnRenamed(\"loyalty_cardholder_acrm_id\", \"customer_rk\")\n",
    "                        .withColumnRenamed(\"loyalty_account_id\", \"account_no\")\n",
    "                        .select('account_no', 'customer_rk')\n",
    "                      )\n",
    "clients_info = loyalty_cards.join(loyalty_cardholders, on='account_no', how='inner')\n",
    "clients_info = clients_info.join(seg_sms, on='customer_rk', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "277270ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_headers_tc5_2 = checks_headers_tc5_2.join(clients_info, on='loyalty_card_no') #оставили чеки только нужных гостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc0996c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_items_2 = (spark.table(CHECKS_ITEMS) \n",
    "                    .withColumnRenamed('plu_id', 'plu_code')\n",
    "                    .withColumnRenamed('turnover_no_vat_amt', 'zsalnovat')\n",
    "                    .withColumnRenamed('turnover_vat_rub_amt', 'zsale_vat')\n",
    "                    .withColumnRenamed('prime_cost_no_vat_amt', 'zcst_n')\n",
    "                    .withColumnRenamed('turnover_base_uom_amt', 'base_qty')\n",
    "                    .withColumnRenamed('discount_amt', 'zdiscount')\n",
    "                    .withColumnRenamed('fact_regular_promo_flg', 'zpromofl')\n",
    "                    .filter(F.col('rtl_txn_dt').between(start_date2, end_date2))\n",
    "                    .filter((F.col('zsalnovat') >= 0) & (F.col('base_qty') >= 0) & (F.col('zcst_n') > 0)) #keep only correct data\n",
    "                        .select('plu_code' #items id\n",
    "                                , 'rtl_txn_id' #cheques id\n",
    "                                , 'base_qty' #count\n",
    "                               )\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3473cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plu_hierarchy_lvl_4_dk = ['FR0405002']\n",
    "syntethic_category_id = [73]\n",
    "# plu_brand_code = ['L473','2945','2657','3624','6390','6416']\n",
    "plu_not_in = [3680376, 3623999, 45105, 3680375, 3624002, 3623997, 3623998, 3624001, 4081735, 3680377, 3179905]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "466c12f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plu_codes_cat_2 = (spark\n",
    "                 .table(PRODUCTS)\n",
    "#                  .filter(F.col('plu_hierarchy_lvl_4_dk').isin(plu_hierarchy_lvl_4_dk))\n",
    "#                  .filter(F.col('plu_brand_code').isin(plu_brand_code))\n",
    "                 .filter(F.col('syntethic_category_id').isin(syntethic_category_id))\n",
    "                 .filter(~F.col('plu_id').isin(plu_not_in))\n",
    "                 .select('plu_id')\n",
    "                 .distinct()\n",
    "                 .toPandas()['plu_id']\n",
    "                 .tolist()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97c4dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_items_2 = checks_items_2.filter(F.col('plu_code').isin(plu_codes_cat_2)) # только чеки с нужными plu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fb79aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks_tc5_2 = checks_items_2.join(checks_headers_tc5_2, 'rtl_txn_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d5565dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = ['account_no']\n",
    "pdf = checks_tc5_2.groupby(accs).agg(F.sum('base_qty').alias('base_qty_per_period'))\n",
    "pdf = pdf.filter(F.col('base_qty_per_period') >= base_qty_per_period)\n",
    "checks_tc5_2 = checks_tc5_2.join(pdf, on=accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "deea2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg2 = (checks_tc5_2.select('customer_rk')\n",
    "                  .distinct()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "feaef617",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = seg1.join(seg2, 'customer_rk', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8efab9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                6957]]7]]]\r"
     ]
    }
   ],
   "source": [
    "seg.write.parquet('temp1961_2', mode='overwrite')\n",
    "seg = spark.read.parquet('temp1961_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfaca492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "182035"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a616ab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "seg_pd = seg.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91b10eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182035"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seg_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31fa88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_no = 'seg_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b75799ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_pd.to_csv(name + '_' + seg_no + '_All_uc' + str(dt) + '.csv', index=False)\n",
    "seg_pd = seg_pd.astype('str')\n",
    "cross, upgrade, ump = on_usecases(seg_pd)\n",
    "cross.to_csv(name + '_' + seg_no + '_' + usecase[0] + '_' + str(dt) + '.csv', index=False)\n",
    "upgrade.to_csv(name + '_' + seg_no + '_' + usecase[1] + '_' + str(dt) + '.csv', index=False)\n",
    "ump.to_csv(name + '_' + seg_no + '_' + usecase[2] + '_' + str(dt) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cec69a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross: 70421, upgrade: 84238, ump: 27376\n"
     ]
    }
   ],
   "source": [
    "print('cross: {}, upgrade: {}, ump: {}'.format(len(cross), len(upgrade), len(ump)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373dffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
