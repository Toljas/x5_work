{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf4c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'tests'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290fd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import List, Dict, Callable\n",
    "import socket\n",
    "\n",
    "spark = None\n",
    "\n",
    "EXECUTOR_ENV = 'hdfs:///share/products/cvm5/lib/python/anaconda_2.4.4_ds.tar.gz'  # 2.4.4 \n",
    "SPARK_ARCHIVE = 'hdfs:///share/lib/spark/sparkjars-2.4.4.zip'                     # 2.4.4\n",
    "#EXECUTOR_ENV = 'hdfs:///share/lib/python/env/anaconda-2019.07.tar.gz'\n",
    "#SPARK_ARCHIVE = 'hdfs:///share/lib/spark/sparkjars-2.3.1.zip'\n",
    "\n",
    "os.environ[\"ARROW_LIBHDFS_DIR\"] = \"/usr/hdp/2.6.5.0-292/usr/lib\"\n",
    "os.environ['HADOOP_HOME'] = '/usr/hdp/current/hadoop-client/'\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64/'\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf/'\n",
    "os.environ['SPARK_HOME'] = '/opt/conda/lib/python3.7/site-packages/pyspark'\n",
    "os.environ['PYSPARK_PYTHON'] = 'anaconda_2.4.4_ds.tar.gz/bin/python3'             # 2.4.4\n",
    "#os.environ['PYSPARK_PYTHON'] = 'anaconda-2019.07.tar.gz/bin/python3'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def restart_spark(task_name: str, num_executors: int, executor_memory='4G', executor_cores=2,\n",
    "                  driver_memory='2G', queue='cvm5-rnd', additional_params: Dict[str, str] = None):\n",
    "    global spark\n",
    "\n",
    "    if spark:\n",
    "        sc = spark.sparkContext\n",
    "        if sc and sc._jsc:\n",
    "            if not sc._jsc.sc().isStopped():\n",
    "                print('Using cached spark')\n",
    "                return sc, spark\n",
    "\n",
    "    need_ports_for_app = 3\n",
    "    user_tcp_ports = _get_user_tcp_ports()\n",
    "    free_ports = _get_free_ports(user_tcp_ports)\n",
    "    assert len(free_ports) >= need_ports_for_app, \\\n",
    "        f\"Not enough free ports ({len(free_ports)}), need {need_ports_for_app}, stop other apps\"\n",
    "    app_ports = free_ports[:need_ports_for_app]\n",
    "\n",
    "    host_ip = os.getenv('HOST_IP')\n",
    "    \n",
    "    spark_session = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(task_name)\n",
    "        .master('yarn')\n",
    "        .config('spark.driver.memory', driver_memory)\n",
    "        .config('spark.driver.maxResultSize', driver_memory)\n",
    "        .config('spark.executor.cores', executor_cores)\n",
    "        .config('spark.executor.memory', executor_memory)\n",
    "        .config('spark.executor.memoryOverhead', '1G')\n",
    "        .config('spark.dynamicAllocation.enabled', 'true')\n",
    "        .config('spark.dynamicAllocation.maxExecutors', num_executors)\n",
    "        .config('spark.sql.broadcastTimeout', '36000')\n",
    "        .config('spark.dynamicAllocation.cachedExecutorIdleTimeout', '1200s')\n",
    "        .config('spark.ui.port', app_ports[0])\n",
    "        .config('spark.blockManager.port', app_ports[1])\n",
    "        .config('spark.driver.port', app_ports[2])\n",
    "        .config('spark.driver.host', host_ip)\n",
    "        .config('spark.driver.bindAddress', '0.0.0.0')\n",
    "        .config('spark.driver.extraLibraryPath', '/usr/hdp/2.6.5.0-292/hadoop/lib/native')\n",
    "        .config('spark.driver.extraJavaOptions', '-Dhdp.version=current')\n",
    "        .config('spark.debug.maxToStringFields', '50')\n",
    "        .config('spark.yarn.queue', queue)\n",
    "        .config('spark.yarn.dist.archives', EXECUTOR_ENV)\n",
    "        .config('spark.yarn.archive', SPARK_ARCHIVE)\n",
    "        .config('spark.yarn.am.extraJavaOptions', '-Dhdp.version=current')\n",
    "        .config('spark.rpc.message.maxSize', '1024')\n",
    "        .config('spark.sql.warehouse.dir', '/apps/hive/warehouse')\n",
    "        .config('spark.sql.execution.pandas.respectSessionTimeZone', 'false')\n",
    "        .config('spark.sql.orc.filterPushdown', 'true')\n",
    "        .config('spark.sql.hive.convertMetastoreOrc', 'true')\n",
    "        .config('spark.shuffle.service.enabled', 'true')\n",
    "        .config('spark.hadoop.yarn.timeline-service.enabled', 'false')\n",
    "        .config('spark.hadoop.yarn.client.failover-proxy-provider',\n",
    "                'org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider')\n",
    "        .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "        .config('spark.kryoserializer.buffer.max', '128m')\n",
    "        .config('spark.executor.extraLibraryPath', '/usr/hdp/2.6.5.0-292/hadoop/lib/native')\n",
    "    )\n",
    "\n",
    "    if additional_params:\n",
    "        for key, value in additional_params.items():\n",
    "            spark_session = spark_session.config(key, value)\n",
    "\n",
    "    spark = (\n",
    "        spark_session\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    return sc, spark\n",
    "\n",
    "\n",
    "def _get_user_tcp_ports() -> List[str]:\n",
    "    regexp = re.compile(r'-2e')\n",
    "    envuser= os.getenv('HOSTNAME')\n",
    "    if regexp.search(envuser):       \n",
    "      _, user_name, user_surname = envuser.upper().split('-')\n",
    "      user_full_name = '_'.join([user_name, user_surname])\n",
    "    else:\n",
    "      _, user_name  = envuser.upper().split('-') \n",
    "      user_full_name = user_name\n",
    "    user_tcp_ports = [v for k, v in os.environ.items() if user_full_name in k and k.endswith('TCP_PORT')]\n",
    "    return user_tcp_ports\n",
    "\n",
    "\n",
    "def _get_free_ports(ports: List[str]):\n",
    "    free_ports = []\n",
    "    for port in ports:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('0.0.0.0', int(port))) != 0:\n",
    "                free_ports.append(port)\n",
    "    return free_ports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba8f0c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc, spark = restart_spark(\n",
    "                    name, \n",
    "                    21, \n",
    "                    executor_memory='5G', \n",
    "                    executor_cores=3, \n",
    "                    driver_memory='7G', \n",
    "                    additional_params={\"spark.sql.shuffle.partitions\": \"300\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51952768",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ec5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "sys.path.append('/home/jovyan/glow-byte-filters-pyspark')\n",
    "from logic_filters import * \n",
    "from segmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203797bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOYALTY_CARDS = \"hive_ssa_tc5.loyalty_card\"\n",
    "LOYALTY_CARDHOLDERS = \"hive_ssa_tc5.loyalty_cardholder\"\n",
    "ACCOUNTS = \"hive_ssa_tc5.account\"\n",
    "CVM5_GUESTS = \"hive_cvm_acrm.cvm5_guests\"\n",
    "\n",
    "DIM_STORE = \"hive_ssa_main.dim_store\"\n",
    "CHECKS_HEADERS = \"hive_ssa_main.fct_rtl_txn\"\n",
    "CHECKS_ITEMS = \"hive_ssa_main.fct_rtl_txn_item\"\n",
    "PRODUCTS = \"hive_ssa_tc5.cvm_product\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a45ae",
   "metadata": {},
   "source": [
    "### Выбираем гостей нужного юзкейса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3daf118",
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase = ['churn', 'frequency', 'cross', 'upgrade', 'ump']\n",
    "dt = datetime.date(2021, 12, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03a8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_usecase = (spark\n",
    "                     .table(CVM5_GUESTS)\n",
    "                     .filter(F.to_date('calculation_dt') == dt)\n",
    "                     .filter(F.col('usecase').isin(usecase))\n",
    "                     .select('account_no', 'customer_rk')\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae74a50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49418569"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_usecase.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c4a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_sms.write.parquet('temp2398_1', mode='overwrite')\n",
    "seg_sms = spark.read.parquet('temp2398_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80f1e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25145153"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_sms.count() #24 145 153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17c4b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite_store = spark.sql('''\n",
    "with last_purch_tab as (\n",
    "select l.loyalty_cardholder_acrm_id as customer_rk, max(checks.rtl_txn_start_dttm) as last_purch\n",
    "from hive_ssa_main.fct_rtl_txn checks\n",
    "join (select ch.loyalty_cardholder_acrm_id, c.loyalty_card_id from hive_ssa_tc5.loyalty_card c left join hive_ssa_tc5.loyalty_cardholder ch\n",
    "on c.loyalty_account_id = ch.loyalty_account_id) l ON checks.loyalty_card_no = l.loyalty_card_id\n",
    "where checks.financial_unit_format_dk ='D' and checks.rtl_txn_cancel_flg = 0\n",
    "group by l.loyalty_cardholder_acrm_id\n",
    "),\n",
    "count_purch_tab as (\n",
    "select l.loyalty_cardholder_acrm_id as customer_rk, checks.store_id, count(checks.rtl_txn_id) as count_purch,\n",
    "max(checks.rtl_txn_start_dttm) as last_purch_dt\n",
    "from hive_ssa_main.fct_rtl_txn checks\n",
    "join (select ch.loyalty_cardholder_acrm_id, c.loyalty_card_id from hive_ssa_tc5.loyalty_card c\n",
    "    left join hive_ssa_tc5.loyalty_cardholder ch\n",
    "    on c.loyalty_account_id = ch.loyalty_account_id) l ON checks.loyalty_card_no = l.loyalty_card_id\n",
    "join last_purch_tab lpt on lpt.customer_rk = l.loyalty_cardholder_acrm_id\n",
    "where checks.financial_unit_format_dk ='D' and checks.rtl_txn_cancel_flg = 0 and\n",
    "datediff(lpt.last_purch, checks.rtl_txn_dt) between 0 and 30\n",
    "group by l.loyalty_cardholder_acrm_id, checks.store_id\n",
    "),\n",
    "max_purch_tab as (select customer_rk, max(count_purch) as max_count_purch, max(last_purch_dt) as last_purch\n",
    "from count_purch_tab\n",
    "group by customer_rk\n",
    ")\n",
    "select mpt.customer_rk, cpt.store_id\n",
    "from max_purch_tab mpt\n",
    "join count_purch_tab cpt on mpt.customer_rk = cpt.customer_rk and mpt.max_count_purch = cpt.count_purch\n",
    "and mpt.last_purch = cpt.last_purch_dt\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18ea3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_users = seg_sms.join(favorite_store, 'customer_rk', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1370077",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_spb = (\n",
    "    spark.table(DIM_STORE)\n",
    "         .filter(F.col('federal_subject_dk').isin(['78']))\n",
    "         .withColumn('federal_subject', F.lit('spb'))\n",
    "         .select('store_id', 'federal_subject')\n",
    ")\n",
    "stores_msk = (\n",
    "    spark.table(DIM_STORE)\n",
    "         .filter(F.col('federal_subject_dk').isin(['77']))\n",
    "         .withColumn('federal_subject', F.lit('msk'))\n",
    "         .select('store_id', 'federal_subject')\n",
    ")\n",
    "stores_ug = (\n",
    "    spark.table(DIM_STORE)\n",
    "         .filter(F.col('macroregion_dk').isin(['MRDUG']))\n",
    "         .withColumn('federal_subject', F.lit('ug'))\n",
    "         .select('store_id', 'federal_subject')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f007a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = stores_spb.union(stores_msk).union(stores_ug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3536a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_people = (\n",
    "      active_users.join(stores, 'store_id', 'inner')\n",
    "                  .groupby('federal_subject').agg(F.countDistinct('customer_rk').alias('qty_customers'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc1a8d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                4454]]4]]]\r"
     ]
    }
   ],
   "source": [
    "count_people_pd = count_people.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d74f9a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>federal_subject</th>\n",
       "      <th>qty_customers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ug</td>\n",
       "      <td>2847095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>msk</td>\n",
       "      <td>1810295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spb</td>\n",
       "      <td>1014377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  federal_subject  qty_customers\n",
       "0              ug        2847095\n",
       "1             msk        1810295\n",
       "2             spb        1014377"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_people_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b15c3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_people_pd.to_csv('count_people.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
